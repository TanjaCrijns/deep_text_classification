{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network on to predict movie likeability based on text reviews.\n",
    "\n",
    "![](images/pos_neg_sentiment.png)\n",
    "\n",
    "Adapted from: \n",
    "- https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py\n",
    "- http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load environment.yml\n",
    "# Instructions to run:\n",
    "# 1. cd to folder containing this file\n",
    "# 2. $ conda env create --force\n",
    "# 3. $ source activate dl\n",
    "\n",
    "name: dl\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - damianavila82\n",
    "dependencies:\n",
    "  - python == 3.6\n",
    "  - jupyter\n",
    "  - rise\n",
    "  - pip:\n",
    "    - tensorflow == 1.0\n",
    "    - keras == 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), often referred to as the IMDB dataset.\n",
    "\n",
    "![](https://kaggle2.blob.core.windows.net/competitions/inclass/4996/media/Screen%20Shot%202016-02-23%20at%2010.56.44%20AM.png)\n",
    "\n",
    "Dataset of 25,000 movies reviews from [http://www.imdb.com/](imbd.com), labeled by sentiment (positive/negative). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">I wish i had more hands, because then I would have more thumbs, because this movie is so terrible because then i could give it so many thumbs down that thumbs down would no longer mean anything because this movie is so terrible because it sucks so badly that it made me laugh out of frustration about the story line because it just would not end because the firing and yelling just kept happening.\n",
    "\n",
    "__Positive or Negative?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Lorenzo Lamas is awesome here, and while he isn't required to act, he is quite fun to watch, and has a really cool character, and had a lot of charisma, however even he can't save this one,and he had no chemistry with the cast either! (Lamas Rules!). \n",
    "\n",
    "__Positive or Negative?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more: https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "25,000 train sequences\n",
      "25,000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "num_words = 1_000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words,\n",
    "                                                      skip_top=25)\n",
    "\n",
    "print('Data loaded.')\n",
    "\n",
    "print(f'{len(x_train):,} train sequences')\n",
    "print(f'{len(x_test):,} test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "\n",
    "For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "\n",
    "This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "![](https://www.davidtan.org/wp-content/uploads/2008/08/review-thumbs-up-down-logo-icon.jpg)\n",
    "\n",
    "The goal is to determine whether a given movie review has a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of categories: \", len(set(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will bound reviews at 500 words, truncating longer reviews and zero-padding shorter reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Now we can define a Deep Learning model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model built\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()                         # A linear stack of layers\n",
    "model.add(Embedding(input_dim=num_words,     # Input layer turns positive integers (indexes) into dense vectors of fixed size\n",
    "                    output_dim= 32, \n",
    "                    input_length=max_words)) \n",
    "model.add(Flatten())                         # Change shape to make linear algebra work\n",
    "model.add(Activation('relu'))                # Add non-linearity\n",
    "model.add(Dropout(0.5))                      # Randomly prune so the model does not overfit\n",
    "model.add(Dense(1))                          # Full connected layer\n",
    "model.add(Activation('sigmoid'))             # Output layer is probability of one of two categories\n",
    "print('Model built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           32000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16001     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 48,001.0\n",
      "Trainable params: 48,001.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', # Loss measures how wrong you are; want this to be small\n",
    "              optimizer='adam',           # How to search the space for best weights; Adam adapts over time\n",
    "              metrics=['accuracy'])       # How to measure success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 16s - loss: 0.6149 - acc: 0.6622 - val_loss: 0.4765 - val_acc: 0.8092\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 11s - loss: 0.4069 - acc: 0.8298 - val_loss: 0.3821 - val_acc: 0.8408\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 13s - loss: 0.3397 - acc: 0.8590 - val_loss: 0.3625 - val_acc: 0.8512\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 12s - loss: 0.3107 - acc: 0.8709 - val_loss: 0.3532 - val_acc: 0.8532\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 12s - loss: 0.2904 - acc: 0.8795 - val_loss: 0.3530 - val_acc: 0.8576\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24640/25000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, \n",
    "                       y_test,\n",
    "                       batch_size=32, \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 85.93%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy: {score[1]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://imgs.xkcd.com/comics/machine_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This data was collected by Stanford researchers and was used in a 2011 paper where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
    "\n",
    "The data was also used as the basis for a Kaggle competition titled [“Bag of Words Meets Bags of Popcorn”](https://www.kaggle.com/c/word2vec-nlp-tutorial) in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
