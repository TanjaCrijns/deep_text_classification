{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate a Multi-Layer Perceptron on movie ratings.\n",
    "\n",
    "![](images/pos_neg_sentiment.png)\n",
    "\n",
    "Adapted from: \n",
    "- https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py\n",
    "- http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is the Large Movie Review Dataset, often referred to as the IMDB dataset.\n",
    "\n",
    "![](https://kaggle2.blob.core.windows.net/competitions/inclass/4996/media/Screen%20Shot%202016-02-23%20at%2010.56.44%20AM.png)\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). \n",
    "\n",
    "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly polar moving reviews (good or bad) for training and the same amount again for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more: https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "25,000 train sequences\n",
      "25,000 test sequences\n",
      "\n",
      "x_train shape: (25000,)\n",
      "x_test shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "num_words = 1_000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words,\n",
    "                                                      skip_top=25)\n",
    "\n",
    "print('Data loaded.')\n",
    "\n",
    "print(f'{len(x_train):,} train sequences')\n",
    "print(f'{len(x_test):,} test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the first few rows of the training dataset:\n",
    "![](https://sandipanweb.files.wordpress.com/2017/03/im33.png)\n",
    "\n",
    "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "\n",
    "For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "\n",
    "This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is to determine whether a given moving review has a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of categories: \", len(set(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will bound reviews at 500 words, truncating longer reviews and zero-padding shorter reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model built\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential() # A linear stack of layers\n",
    "model.add(Embedding(input_dim=num_words, # Input layer turns positive integers (indexes) into dense vectors of fixed size\n",
    "                    output_dim= 32, \n",
    "                    input_length=max_words)) \n",
    "model.add(Flatten()) # Change shape to make linear algebra work\n",
    "model.add(Activation('relu')) # Add non-linearity\n",
    "model.add(Dropout(0.5)) # Randomly prune so the model does not overfit\n",
    "model.add(Dense(1)) # Full connected layer\n",
    "model.add(Activation('sigmoid')) # Output layer is probability of one of two categories\n",
    "print('Model built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 500, 32)           32000     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 16001     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 48,001.0\n",
      "Trainable params: 48,001.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', # Loss measures how wrong you are; want this to be small\n",
    "              optimizer='adam',           # How to search the space for best weights; Adam adapts over time\n",
    "              metrics=['accuracy'])       # How to measure success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 3s - loss: 0.5994 - acc: 0.6823 - val_loss: 0.4592 - val_acc: 0.8152\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 2s - loss: 0.3952 - acc: 0.8357 - val_loss: 0.3726 - val_acc: 0.8476\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 2s - loss: 0.3343 - acc: 0.8592 - val_loss: 0.3509 - val_acc: 0.8560\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 2s - loss: 0.3055 - acc: 0.8721 - val_loss: 0.3448 - val_acc: 0.8592\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 2s - loss: 0.2855 - acc: 0.8831 - val_loss: 0.3423 - val_acc: 0.8656\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24128/25000 [===========================>..] - ETA: 0sTest score: 0.321120530834\n",
      "Test accuracy: 0.86148\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, \n",
    "                       y_test,\n",
    "                       batch_size=32, \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.15%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy: {score[1]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This data was collected by Stanford researchers and was used in a 2011 paper where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
    "\n",
    "The data was also used as the basis for a Kaggle competition titled [“Bag of Words Meets Bags of Popcorn”](https://www.kaggle.com/c/word2vec-nlp-tutorial) in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
